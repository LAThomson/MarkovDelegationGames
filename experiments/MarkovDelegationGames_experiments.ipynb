{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjRnlRuH1654"
      },
      "source": [
        "# Markov Delegation Games Experiments\n",
        "\n",
        "This notebook contains the code for running experiments for my fourth-year project, _Measuring Cooperation and Control in Markov Delegation Games_. I implement a new environment within the JaxMARL framework called _Fruit Salad_ and train agents on this environment, calculating various measures as detailed in this project's theoretical report.\n",
        "\n",
        "‚ö†Ô∏è Ensure you select a GPU from `Runtime > Change runtime type` ‚ö†Ô∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qy-SX12H5_"
      },
      "source": [
        "### Install dependencies üìö\n",
        "\n",
        "We install JAX for use with a GPU, there is rather a lot to download so this may take a second or two.\n",
        "\n",
        "Note, that the runtime will reboot automatically immediately after.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJIaqAt40_rg",
        "outputId": "553cb4c2-d711-498a-e5e2-26c758230fa7"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -qq \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install -qq matplotlib pettingzoo\n",
        "exit(0) #essentially, restarts the runtime after the installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOaiuWkePf7f"
      },
      "source": [
        "## Part 1: JaxMARL API üïπÔ∏è\n",
        "\n",
        "Our API is inspired by [PettingZoo](https://github.com/Farama-Foundation/PettingZoo) and [Gymnax](https://github.com/RobertTLange/gymnax), making it familiar to MARL researchers. Below, an MPE scenario is instatiated from JaxMARL's registry and a trajectory is sampled using random actions. We then visualise the results. Examples for more JaxMARL environments can be found [here](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/tutorials).\n",
        "\n",
        "* `actions`, `obs`, `rewards`, `dones` are dictionaries keyed by agent name, this allows for differing action and observation spaces. As agents can terminate asychronously, `dones` contains a special `\"__all__\"` which signifies whether an episode has terminated.\n",
        "* `state` represents the internal state of the environment and contains all the information needed to transistion the environment given a set of actions. These variables are not held within the environment class due to JAX transformations requiring pure functions.\n",
        "* `info` is a dictionary containing pertinent information, the exact content varies environment to environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h3VkMmsdPfc0",
        "outputId": "4d215fa4-df2b-4ce8-af01-ba317cdfdab0"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import jax\n",
        "from jaxmarl import make\n",
        "from jaxmarl.environments.fruit_salad import FruitSalad, fruit_salad_layouts\n",
        "\n",
        "# Parameters + random keys\n",
        "num_agents = 4\n",
        "max_steps = 25\n",
        "layout = fruit_salad_layouts[\"compass_4p\"]\n",
        "key = jax.random.PRNGKey(0)\n",
        "key, key_r, key_a = jax.random.split(key, 3)\n",
        "\n",
        "# Instantiate environment\n",
        "env = make('fruit_salad', num_agents=num_agents, max_steps=max_steps, layout=layout)\n",
        "obs, state = env.reset(key_r)\n",
        "print('list of agents in environment', env.agents)\n",
        "\n",
        "# Sample random actions\n",
        "key_a = jax.random.split(key_a, env.num_agents)\n",
        "actions = {agent: env.action_space(agent).sample(key_a[i]) for i, agent in enumerate(env.agents)}\n",
        "print('example action dict', actions)\n",
        "\n",
        "# Collect trajectory\n",
        "state_seq = []\n",
        "for _ in range(max_steps):\n",
        "    state_seq.append(state)\n",
        "    # Iterate random keys and sample actions\n",
        "    key, key_s, key_a = jax.random.split(key, 3)\n",
        "    key_a = jax.random.split(key_a, env.num_agents)\n",
        "    actions = {agent: env.action_space(agent).sample(key_a[i]) for i, agent in enumerate(env.agents)}\n",
        "\n",
        "    # Step environment\n",
        "    # print(f\"Outer Step: {actions=}\")\n",
        "    obs, state, rewards, dones, infos = env.step(key_s, state, actions)\n",
        "\n",
        "print(state_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAIcs7kw2ROo"
      },
      "source": [
        "## Part 2: Compare speed üöÄ\n",
        "\n",
        "We can then compare the speed of JaxMARL's MPE environments to the CPU-based ones provided by PettingZoo. We again take random actions and compare the number of environment steps per second between the two implementations.\n",
        "\n",
        "We also show how JaxMARL environments can be simply vectorised, using `jax.vmap`, to greatly increase the number of environment steps per second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_y9AyBcUU8v"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"fruit_salad\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRhtSTpYRg83"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jaxmarl\n",
        "import jax\n",
        "\n",
        "def make_benchmark(config):\n",
        "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
        "    config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
        "\n",
        "    def benchmark(rng):\n",
        "        def init_runner_state(rng):\n",
        "\n",
        "            # INIT ENV\n",
        "            rng, _rng = jax.random.split(rng)\n",
        "            reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "            obsv, env_state = jax.vmap(env.reset)(reset_rng)\n",
        "\n",
        "            return (env_state, obsv, rng)\n",
        "\n",
        "        def env_step(runner_state, unused):\n",
        "            env_state, last_obs, rng = runner_state\n",
        "\n",
        "            # SELECT ACTION\n",
        "            rng, _rng = jax.random.split(rng)\n",
        "            rngs = jax.random.split(_rng, config[\"NUM_ACTORS\"]).reshape((env.num_agents, config[\"NUM_ENVS\"], -1))\n",
        "            actions = {k: jax.vmap(env.action_space(k).sample)(rngs[i]) for i, k in enumerate(env.agents)}\n",
        "\n",
        "            # STEP ENV\n",
        "            rng, _rng = jax.random.split(rng)\n",
        "            rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "            obsv, env_state, _, _, info = jax.vmap(env.step)(\n",
        "                rng_step, env_state, actions\n",
        "            )\n",
        "            runner_state = (env_state, obsv, rng)\n",
        "            return runner_state, None\n",
        "\n",
        "        rng, init_rng = jax.random.split(rng)\n",
        "        runner_state = init_runner_state(init_rng)\n",
        "        runner_state = jax.lax.scan(env_step, runner_state, None, config[\"NUM_STEPS\"])\n",
        "        return runner_state\n",
        "\n",
        "    return benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9VtVKgbRyMQ",
        "outputId": "3d680b6f-92f2-41db-8596-7f6f75d2a4d8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxmarl.environments.mpe.simple_world_comm import SimpleWorldCommMPE\n",
        "from pettingzoo.mpe import simple_v3, simple_tag_v3, simple_world_comm_v3, simple_reference_v3, simple_spread_v3, simple_crypto_v3, simple_speaker_listener_v4, simple_push_v3, simple_adversary_v3\n",
        "import time\n",
        "\n",
        "config = {\n",
        "    \"NUM_STEPS\": 1000,\n",
        "    \"NUM_ENVS\": 1000,\n",
        "    \"ACTIVATION\": \"relu\",\n",
        "    \"ENV_KWARGS\": {},\n",
        "    \"ENV_NAME\": ENV_NAME,\n",
        "    \"NUM_SEEDS\": 1,\n",
        "    \"SEED\": 0,\n",
        "}\n",
        "\n",
        "### JAXMARL BENCHMARK\n",
        "num_envs = [1, 100, 1000, 10000]\n",
        "jaxmarl_sps = []\n",
        "for num in num_envs:\n",
        "  config[\"NUM_ENVS\"] = num\n",
        "  benchmark_fn = jax.jit(make_benchmark(config))\n",
        "  rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "  rng, _rng = jax.random.split(rng)\n",
        "  benchmark_jit = jax.jit(benchmark_fn).lower(_rng).compile()\n",
        "  before = time.perf_counter_ns()\n",
        "  runner_state = jax.block_until_ready(benchmark_jit(_rng))\n",
        "  after = time.perf_counter_ns()\n",
        "  total_time = (after - before) / 1e9\n",
        "\n",
        "  sps = config['NUM_STEPS'] * config['NUM_ENVS'] / total_time\n",
        "  jaxmarl_sps.append(sps)\n",
        "\n",
        "  print(f\"JaxMARL, Num Envs: {num}, Total Time (s): {total_time}\")\n",
        "  print(f\"JaxMARL, Num Envs: {num}, Total Steps: {config['NUM_STEPS'] * config['NUM_ENVS']}\")\n",
        "  print(f\"JaxMARL, Num Envs: {num}, SPS: {sps}\")\n",
        "\n",
        "\n",
        "### PETTING ZOO BENCHMARK\n",
        "zoo_mpe_env_mapper = {\n",
        "    \"MPE_simple_v3\": simple_v3,\n",
        "    \"MPE_simple_world_comm_v3\": simple_world_comm_v3,\n",
        "    \"MPE_simple_tag_v3\": simple_tag_v3,\n",
        "    \"MPE_simple_spread_v3\": simple_spread_v3,\n",
        "    \"MPE_simple_crypto_v3\": simple_crypto_v3,\n",
        "    \"MPE_simple_speaker_listener_v4\": simple_speaker_listener_v4,\n",
        "    \"MPE_simple_push_v3\": simple_push_v3,\n",
        "    \"MPE_simple_adversary_v3\": simple_adversary_v3,\n",
        "    \"MPE_simple_reference_v3\": simple_reference_v3,\n",
        "}\n",
        "zoo_env = zoo_mpe_env_mapper[config[\"ENV_NAME\"]]\n",
        "env = zoo_env.parallel_env(max_cycles=max_steps)\n",
        "obs = env.reset()\n",
        "\n",
        "start_time = time.time()\n",
        "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
        "\n",
        "for _ in range(config[\"NUM_STEPS\"]):\n",
        "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}  # this is where you would insert your policy\n",
        "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
        "\n",
        "zoo_time = time.time() - start_time\n",
        "zoo_sps = config[\"NUM_STEPS\"]/zoo_time\n",
        "\n",
        "\n",
        "print(f\"PettingZoo Total Time (s): {zoo_time}\")\n",
        "print(f\"PettingZoo Total Steps: {config['NUM_STEPS']}\")\n",
        "print(f\"PettingZoo SPS: {zoo_sps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "6YLL7Aj-ShER",
        "outputId": "479f72ba-cf75-4796-9577-7bf37d7d0a13"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(num_envs, jaxmarl_sps, linestyle='--', marker='o', label=\"JAX\")\n",
        "plt.axhline(y=zoo_sps, color='r', linestyle='--', label=\"PettingZoo\")\n",
        "plt.legend()\n",
        "\n",
        "plt.ylabel(\"Steps per Second\")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel(\"Number of parallel environments\")\n",
        "plt.title(f\"Steps per second for {MPE_ENV}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WobTKkl25IlM"
      },
      "source": [
        "## Part 3: Train an agent üç≤\n",
        "\n",
        "We train an agent on the `cramped_room` Overcooked scenario using IPPO, our IPPO code is based off [PureJaxRL](https://github.com/luchris429/purejaxrl). Here we also show how `jax.vmap` can again be used to simply train over multiple seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGwk68jK2VPK"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any\n",
        "from flax.training.train_state import TrainState\n",
        "import distrax\n",
        "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
        "import jaxmarl\n",
        "from jaxmarl.wrappers.baselines import LogWrapper\n",
        "from jaxmarl.environments.overcooked import overcooked_layouts\n",
        "from jaxmarl.viz.overcooked_visualizer import OvercookedVisualizer\n",
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    action_dim: Sequence[int]\n",
        "    activation: str = \"tanh\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        if self.activation == \"relu\":\n",
        "            activation = nn.relu\n",
        "        else:\n",
        "            activation = nn.tanh\n",
        "        actor_mean = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(x)\n",
        "        actor_mean = activation(actor_mean)\n",
        "        actor_mean = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(actor_mean)\n",
        "        actor_mean = activation(actor_mean)\n",
        "        actor_mean = nn.Dense(\n",
        "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
        "        )(actor_mean)\n",
        "        pi = distrax.Categorical(logits=actor_mean)\n",
        "\n",
        "        critic = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(x)\n",
        "        critic = activation(critic)\n",
        "        critic = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(critic)\n",
        "        critic = activation(critic)\n",
        "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
        "            critic\n",
        "        )\n",
        "\n",
        "        return pi, jnp.squeeze(critic, axis=-1)\n",
        "\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    done: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    value: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    log_prob: jnp.ndarray\n",
        "    obs: jnp.ndarray\n",
        "    info: jnp.ndarray\n",
        "\n",
        "def get_rollout(train_state, config):\n",
        "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
        "\n",
        "    network = ActorCritic(env.action_space().n, activation=config[\"ACTIVATION\"])\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    key, key_r, key_a = jax.random.split(key, 3)\n",
        "\n",
        "    init_x = jnp.zeros(env.observation_space().shape)\n",
        "    init_x = init_x.flatten()\n",
        "\n",
        "    network.init(key_a, init_x)\n",
        "    network_params = train_state.params\n",
        "\n",
        "    done = False\n",
        "\n",
        "    obs, state = env.reset(key_r)\n",
        "    state_seq = [state]\n",
        "    while not done:\n",
        "        key, key_a0, key_a1, key_s = jax.random.split(key, 4)\n",
        "\n",
        "        # obs_batch = batchify(obs, env.agents, config[\"NUM_ACTORS\"])\n",
        "        # breakpoint()\n",
        "        obs = {k: v.flatten() for k, v in obs.items()}\n",
        "\n",
        "        pi_0, _ = network.apply(network_params, obs[\"agent_0\"])\n",
        "        pi_1, _ = network.apply(network_params, obs[\"agent_1\"])\n",
        "\n",
        "        actions = {\"agent_0\": pi_0.sample(seed=key_a0), \"agent_1\": pi_1.sample(seed=key_a1)}\n",
        "        # env_act = unbatchify(action, env.agents, config[\"NUM_ENVS\"], env.num_agents)\n",
        "        # env_act = {k: v.flatten() for k, v in env_act.items()}\n",
        "\n",
        "        # STEP ENV\n",
        "        obs, state, reward, done, info = env.step(key_s, state, actions)\n",
        "        done = done[\"__all__\"]\n",
        "\n",
        "        state_seq.append(state)\n",
        "\n",
        "    return state_seq\n",
        "\n",
        "def batchify(x: dict, agent_list, num_actors):\n",
        "    x = jnp.stack([x[a] for a in agent_list])\n",
        "    return x.reshape((num_actors, -1))\n",
        "\n",
        "\n",
        "def unbatchify(x: jnp.ndarray, agent_list, num_envs, num_actors):\n",
        "    x = x.reshape((num_actors, num_envs, -1))\n",
        "    return {a: x[i] for i, a in enumerate(agent_list)}\n",
        "\n",
        "def make_train(config):\n",
        "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
        "\n",
        "    config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
        "    config[\"NUM_UPDATES\"] = (\n",
        "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
        "    )\n",
        "    config[\"MINIBATCH_SIZE\"] = (\n",
        "        config[\"NUM_ACTORS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
        "    )\n",
        "\n",
        "    env = LogWrapper(env)\n",
        "\n",
        "    def linear_schedule(count):\n",
        "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
        "        return config[\"LR\"] * frac\n",
        "\n",
        "    def train(rng):\n",
        "\n",
        "        # INIT NETWORK\n",
        "        network = ActorCritic(env.action_space().n, activation=config[\"ACTIVATION\"])\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        init_x = jnp.zeros(env.observation_space().shape)\n",
        "\n",
        "        init_x = init_x.flatten()\n",
        "\n",
        "        network_params = network.init(_rng, init_x)\n",
        "        if config[\"ANNEAL_LR\"]:\n",
        "            tx = optax.chain(\n",
        "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
        "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
        "            )\n",
        "        else:\n",
        "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
        "        train_state = TrainState.create(\n",
        "            apply_fn=network.apply,\n",
        "            params=network_params,\n",
        "            tx=tx,\n",
        "        )\n",
        "\n",
        "        # INIT ENV\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "        obsv, env_state = jax.vmap(env.reset, in_axes=(0,))(reset_rng)\n",
        "\n",
        "        # TRAIN LOOP\n",
        "        def _update_step(runner_state, unused):\n",
        "            # COLLECT TRAJECTORIES\n",
        "            def _env_step(runner_state, unused):\n",
        "                train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "                # SELECT ACTION\n",
        "                rng, _rng = jax.random.split(rng)\n",
        "\n",
        "                obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
        "\n",
        "                pi, value = network.apply(train_state.params, obs_batch)\n",
        "                action = pi.sample(seed=_rng)\n",
        "                log_prob = pi.log_prob(action)\n",
        "                env_act = unbatchify(action, env.agents, config[\"NUM_ENVS\"], env.num_agents)\n",
        "\n",
        "                env_act = {k:v.flatten() for k,v in env_act.items()}\n",
        "\n",
        "                # STEP ENV\n",
        "                rng, _rng = jax.random.split(rng)\n",
        "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0))(\n",
        "                    rng_step, env_state, env_act\n",
        "                )\n",
        "                info = jax.tree_map(lambda x: x.reshape((config[\"NUM_ACTORS\"])), info)\n",
        "                transition = Transition(\n",
        "                    batchify(done, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
        "                    action,\n",
        "                    value,\n",
        "                    batchify(reward, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
        "                    log_prob,\n",
        "                    obs_batch,\n",
        "                    info\n",
        "\n",
        "                )\n",
        "                runner_state = (train_state, env_state, obsv, rng)\n",
        "                return runner_state, transition\n",
        "\n",
        "            runner_state, traj_batch = jax.lax.scan(\n",
        "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
        "            )\n",
        "\n",
        "            # CALCULATE ADVANTAGE\n",
        "            train_state, env_state, last_obs, rng = runner_state\n",
        "            last_obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
        "            _, last_val = network.apply(train_state.params, last_obs_batch)\n",
        "\n",
        "            def _calculate_gae(traj_batch, last_val):\n",
        "                def _get_advantages(gae_and_next_value, transition):\n",
        "                    gae, next_value = gae_and_next_value\n",
        "                    done, value, reward = (\n",
        "                        transition.done,\n",
        "                        transition.value,\n",
        "                        transition.reward,\n",
        "                    )\n",
        "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
        "                    gae = (\n",
        "                        delta\n",
        "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
        "                    )\n",
        "                    return (gae, value), gae\n",
        "\n",
        "                _, advantages = jax.lax.scan(\n",
        "                    _get_advantages,\n",
        "                    (jnp.zeros_like(last_val), last_val),\n",
        "                    traj_batch,\n",
        "                    reverse=True,\n",
        "                    unroll=16,\n",
        "                )\n",
        "                return advantages, advantages + traj_batch.value\n",
        "\n",
        "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
        "\n",
        "            # UPDATE NETWORK\n",
        "            def _update_epoch(update_state, unused):\n",
        "                def _update_minbatch(train_state, batch_info):\n",
        "                    traj_batch, advantages, targets = batch_info\n",
        "\n",
        "                    def _loss_fn(params, traj_batch, gae, targets):\n",
        "                        # RERUN NETWORK\n",
        "                        pi, value = network.apply(params, traj_batch.obs)\n",
        "                        log_prob = pi.log_prob(traj_batch.action)\n",
        "\n",
        "                        # CALCULATE VALUE LOSS\n",
        "                        value_pred_clipped = traj_batch.value + (\n",
        "                            value - traj_batch.value\n",
        "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
        "                        value_losses = jnp.square(value - targets)\n",
        "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
        "                        value_loss = (\n",
        "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
        "                        )\n",
        "\n",
        "                        # CALCULATE ACTOR LOSS\n",
        "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
        "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
        "                        loss_actor1 = ratio * gae\n",
        "                        loss_actor2 = (\n",
        "                            jnp.clip(\n",
        "                                ratio,\n",
        "                                1.0 - config[\"CLIP_EPS\"],\n",
        "                                1.0 + config[\"CLIP_EPS\"],\n",
        "                            )\n",
        "                            * gae\n",
        "                        )\n",
        "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
        "                        loss_actor = loss_actor.mean()\n",
        "                        entropy = pi.entropy().mean()\n",
        "\n",
        "                        total_loss = (\n",
        "                            loss_actor\n",
        "                            + config[\"VF_COEF\"] * value_loss\n",
        "                            - config[\"ENT_COEF\"] * entropy\n",
        "                        )\n",
        "                        return total_loss, (value_loss, loss_actor, entropy)\n",
        "\n",
        "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
        "                    total_loss, grads = grad_fn(\n",
        "                        train_state.params, traj_batch, advantages, targets\n",
        "                    )\n",
        "                    train_state = train_state.apply_gradients(grads=grads)\n",
        "                    return train_state, total_loss\n",
        "\n",
        "                train_state, traj_batch, advantages, targets, rng = update_state\n",
        "                rng, _rng = jax.random.split(rng)\n",
        "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
        "                assert (\n",
        "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ACTORS\"]\n",
        "                ), \"batch size must be equal to number of steps * number of actors\"\n",
        "                permutation = jax.random.permutation(_rng, batch_size)\n",
        "                batch = (traj_batch, advantages, targets)\n",
        "                batch = jax.tree_util.tree_map(\n",
        "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
        "                )\n",
        "                shuffled_batch = jax.tree_util.tree_map(\n",
        "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
        "                )\n",
        "                minibatches = jax.tree_util.tree_map(\n",
        "                    lambda x: jnp.reshape(\n",
        "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
        "                    ),\n",
        "                    shuffled_batch,\n",
        "                )\n",
        "                train_state, total_loss = jax.lax.scan(\n",
        "                    _update_minbatch, train_state, minibatches\n",
        "                )\n",
        "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
        "                return update_state, total_loss\n",
        "\n",
        "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
        "            update_state, loss_info = jax.lax.scan(\n",
        "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
        "            )\n",
        "            train_state = update_state[0]\n",
        "            metric = traj_batch.info\n",
        "            rng = update_state[-1]\n",
        "\n",
        "            runner_state = (train_state, env_state, last_obs, rng)\n",
        "            return runner_state, metric\n",
        "\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        runner_state = (train_state, env_state, obsv, _rng)\n",
        "        runner_state, metric = jax.lax.scan(\n",
        "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
        "        )\n",
        "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
        "\n",
        "    return train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "AxrH1jHv3orP",
        "outputId": "9b9d8d7e-615d-437a-b9a4-edb9cddd975b"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# set hyperparameters:\n",
        "config = {\n",
        "    \"LR\": 2.5e-4,\n",
        "    \"NUM_ENVS\": 16,\n",
        "    \"NUM_STEPS\": 128,\n",
        "    \"TOTAL_TIMESTEPS\": 5e6,\n",
        "    \"UPDATE_EPOCHS\": 4,\n",
        "    \"NUM_MINIBATCHES\": 4,\n",
        "    \"GAMMA\": 0.99,\n",
        "    \"GAE_LAMBDA\": 0.95,\n",
        "    \"CLIP_EPS\": 0.2,\n",
        "    \"ENT_COEF\": 0.01,\n",
        "    \"VF_COEF\": 0.5,\n",
        "    \"MAX_GRAD_NORM\": 0.5,\n",
        "    \"ACTIVATION\": \"tanh\",\n",
        "    \"ENV_NAME\": \"overcooked\",\n",
        "    \"ENV_KWARGS\": {\n",
        "      \"layout\" : \"cramped_room\"\n",
        "    },\n",
        "    \"ANNEAL_LR\": True,\n",
        "    \"SEED\": 0,\n",
        "    \"NUM_SEEDS\": 3\n",
        "}\n",
        "\n",
        "config[\"ENV_KWARGS\"][\"layout\"] = overcooked_layouts[config[\"ENV_KWARGS\"][\"layout\"]]\n",
        "rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "rngs = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
        "with jax.disable_jit(False):\n",
        "    train_jit = jax.jit(jax.vmap(make_train(config)))\n",
        "    out = train_jit(rngs)\n",
        "\n",
        "\n",
        "for i in range(config[\"NUM_SEEDS\"]):\n",
        "    plt.plot(out[\"metrics\"][\"returned_episode_returns\"][i].mean(-1).reshape(-1))\n",
        "plt.xlabel(\"Update Step\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
